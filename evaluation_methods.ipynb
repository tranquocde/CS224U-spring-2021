{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation methods in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Your projects](#Your-projects)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Data organization](#Data-organization)\n",
    "  1. [Train/dev/test](#Train/dev/test)\n",
    "  1. [No fixed splits](#No-fixed-splits)\n",
    "1. [Cross-validation](#Cross-validation)\n",
    "  1. [Random splits](#Random-splits)\n",
    "  1. [K-folds](#K-folds)\n",
    "1. [Baselines](#Baselines)\n",
    "  1. [Baselines are crucial for strong experiments](#Baselines-are-crucial-for-strong-experiments)\n",
    "  1. [Random baselines](#Random-baselines)\n",
    "  1. [Task-specific baselines](#Task-specific-baselines)\n",
    "1. [Hyperparameter optimization](#Hyperparameter-optimization)\n",
    "  1. [Rationale](#Rationale)\n",
    "  1. [The ideal hyperparameter optimization setting](#The-ideal-hyperparameter-optimization-setting)\n",
    "  1. [Practical considerations, and some compromises](#Practical-considerations,-and-some-compromises)\n",
    "  1. [Hyperparameter optimization tools](#Hyperparameter-optimization-tools)\n",
    "1. [Classifier comparison](#Classifier-comparison)\n",
    "  1. [Practical differences](#Practical-differences)\n",
    "  1. [Confidence intervals](#Confidence-intervals)\n",
    "  1. [Wilcoxon signed-rank test](#Wilcoxon-signed-rank-test)\n",
    "  1. [McNemar's test](#McNemar's-test)\n",
    "1. [Assessing models without convergence](#Assessing-models-without-convergence)\n",
    "  1. [Incremental dev set testing](#Incremental-dev-set-testing)\n",
    "  1. [Learning curves with confidence intervals](#Learning-curves-with-confidence-intervals)\n",
    "1. [The role of random parameter initialization](#The-role-of-random-parameter-initialization)\n",
    "1. [Closing remarks](#Closing-remarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is an overview of experimental methods for NLU. My primary goal is to help you with the experiments you'll be doing for your projects.  It is a companion to [the evaluation metrics notebook](evaluation_metrics.ipynb), which I suggest studying first.\n",
    "\n",
    "The teaching team will be paying special attention to how you conduct your evaluations, so this notebook should create common ground around what our values are.\n",
    "\n",
    "This notebook is far from comprehensive. I hope it covers the most common tools, techniques, and challenges in the field. Beyond that, I'm hoping the examples here suggest a perspective on experiments and evaluations that generalizes to other topics and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Your projects\n",
    "\n",
    "1. We will never evaluate a project based on how \"good\" the results are.\n",
    "  1. Publication venues do this, because they have additional constraints on space that lead them to favor positive evidence for new developments over negative results.\n",
    "  1. In CS224u, we are not subject to this constraint, so we can do the right and good thing of valuing positive results, negative results, and everything in between.\n",
    "\n",
    "1. We __will__ evaluate your project on:   \n",
    "  1. The appropriateness of the metrics\n",
    "  1. The strength of the methods\n",
    "  1. The extent to which the paper is open and clear-sighted about the limits of its findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sst\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train/dev/test\n",
    "\n",
    "Many publicly available datasets are released with a train/dev/test structure. __We're all on the honor system to do test-set runs only when development is complete.__\n",
    "\n",
    "Splits like this basically presuppose a fairly large dataset.\n",
    "\n",
    "If there is no dev set as part of the distribution, then you might create one to simulate what a test run will be like, though you have to weigh this against the reduction in train-set size.\n",
    "\n",
    "Having a fixed test set ensures that all systems are assessed against the same gold data. This is generally good, but it is problematic where the test set turns out to have unusual properties that distort progress on the task. Ideally, every task would have dozens of test sets, so that we could report average performance and related statistics. The difficulty and expense of creating so many test sets means that this ideal is rarely if ever realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### No fixed splits\n",
    "\n",
    "Many datasets are released without predefined splits. This poses challenges for assessment, especially comparative assessment: __for robust comparisons with prior work, you really have to rerun the models using your assessment regime on your splits__. For example, if you're doing [5-fold cross-validation](#K-folds), then all the systems should be trained and assessed using exactly the same folds, to control for variation in how difficult the splits are.\n",
    "\n",
    "If the dataset is large enough, you might create a train/test or train/dev/test split right at the start of your project and use it for all your experiments. This means putting the test portion in a locked box until the very end, when you assess all the relevant systems against it. For large datasets, this will certainly simplify your experimental set-up, for reasons that will become clear when we discuss [hyperparameter optimization](#Hyperparameter-optimization) below.\n",
    "\n",
    "For small datasets, carving out dev and test sets might leave you with too little data. The most problematic symptom of this is that performance is highly variable because there isn't enough data to optimize reliably. In such situations, you might give up on having fixed splits, opting instead for some form of cross-validation, which allows you to average over multiple runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "In cross-validation, we take a set of examples $X$ and partition them into two or more train/test splits, and then we average over the results in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random splits\n",
    "\n",
    "When creating random train/test splits, we shuffle the examples and split them, with a pre-specified percentage $t$ used for training and another pre-specified percentage (usually $1-t$) used for testing.\n",
    "\n",
    "In general, we want these splits to be __stratified__ in the sense that the train and test splits have approximately the same distribution over the classes.\n",
    "\n",
    "#### The good and the bad of random splits\n",
    "\n",
    "A nice thing about random splits is that you can create as many as you want without having this impact the ratio of training to testing examples. \n",
    "\n",
    "This can also be a liability, though, as there's no guarantee that every example will be used the same number of times for training and testing. In principle, one might even evaluate on the same split more than once (though this will be fantastically unlikely for large datasets). NLP datasets are generally large enough that this isn't a pressing concern.\n",
    "\n",
    "The function `utils.fit_classifier_with_hyperparameter_search` hard-codes a strategy of using random splits by using the [StratifiedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit) utility. The benefit of decoupling the train/test ratio from the number of splits outweights the concerns about split composition.\n",
    "\n",
    "#### Random splits in scikit-learn\n",
    "\n",
    "In scikit-learn, the function [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) will do random splits. It is a wrapper around [ShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit) or [StratifiedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit), depending on how the keyword argument `stratify` is used. A potential gotcha for classification problems: `train_test_split` does not stratify its splits by default, whereas stratified splits are desired in most situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-folds\n",
    "\n",
    "In K-fold cross-validation, one divides the data into $k$ folds of equal size and then conducts $k$ experiments. In each, fold $i$ is used for assessment, and all the other folds are merged together for training:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c c c }\n",
    "\\textbf{Splits} & \\textbf{Experiment 1} & \\textbf{Experiment 2} & \\textbf{Experiment 3} \\\\\n",
    "\\begin{array}{|c|}\n",
    "\\hline\n",
    "\\textrm{fold } 1  \\\\\\hline\n",
    "\\textrm{fold } 2  \\\\\\hline\n",
    "\\textrm{fold } 3  \\\\\\hline\n",
    "\\end{array}\n",
    "& \n",
    "\\begin{array}{|c c|}\n",
    "\\hline\n",
    "\\textbf{Test} & \\textrm{fold } 1  \\\\\\hline\n",
    "\\textbf{Train} & \\textrm{fold } 2  \\\\\n",
    "& \\textrm{fold } 3  \\\\\\hline\n",
    "\\end{array}\n",
    "&\n",
    "\\begin{array}{|c c|}\n",
    "\\hline\n",
    "\\textbf{Test} & \\textrm{fold } 2  \\\\\\hline\n",
    "\\textbf{Train} & \\textrm{fold } 1  \\\\\n",
    "& \\textrm{fold } 3  \\\\\\hline\n",
    "\\end{array}\n",
    "&\n",
    "\\begin{array}{|c c|}\n",
    "\\hline\n",
    "\\textbf{Test} & \\textrm{fold } 3  \\\\\\hline\n",
    "\\textbf{Train} & \\textrm{fold } 1  \\\\\n",
    "& \\textrm{fold } 2  \\\\\\hline\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### The good and the bad of k-folds\n",
    "\n",
    "* With k-folds, every example appears in a train set exactly $k-1$ times and in a test set exactly once. We noted above that random splits do not guarantee this.\n",
    "\n",
    "* A major drawback of k-folds is that the size of $k$ determines the size of the train/test splits. With 3-fold cross validation, one trains on 67% of the data and tests on 33%. With 10-fold cross-validation, one trains on 90% and tests on 10%. These are likely to be __very__ different experimental scenarios. This is a consideration one should have in mind when [comparing models](#Classifier-comparison) using statistical tests that depend on repeated runs. This is a large enough drawback to k-folds that I generally favor random splits, [as discussed just above](#Random-splits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### K-folds in scikit-learn\n",
    "\n",
    "* In scikit-learn, [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) and [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) are the primary classes for creating k-folds from a dataset. As with random splits, the stratified option is recommended for most classification problems, as one generally want to train and assess with the same label distribution.\n",
    "\n",
    "* The methods [cross_validate](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) and [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) are convenience methods that let you pass in a model (`estimator`), a dataset (`X` and `y`), and some cross-validation parameters, and they handle the repeated assessments. These are great. Two tips:\n",
    "  * I strongly recommend passing in a `KFold` or `StratifiedKFold` instance as the value of `cv` to ensure that you get the split behavior that you desire.\n",
    "  * Check that `scoring` has the value that you desire. For example, if you are going to report F1-scores, it's a mistake to leave `scoring=None`, as this will default to whatever your model reports with its `score` method, which is probably accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Variants  \n",
    "  \n",
    "K-folds has a number of variants and special cases. Two that frequently arise in NLU:\n",
    "\n",
    "1. [LeaveOneOut](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut) is the special case where the number of folds equals the number of examples. This is especially useful for very small datasets.\n",
    "\n",
    "1. [LeavePGroupsOut](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut) creates folds based on criteria that you define. This is useful in situations where the datasets have important structure that the splits need to respect – e.g., you want to assess against a graph sub-network that is never seen on training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baselines\n",
    "\n",
    "Evaluation numbers in NLP (and throughout AI) __can never be understood properly in isolation__:\n",
    "\n",
    "* If your system gets 0.95 F1, that might seem great in absolute terms, but your readers will suspect the task is too easy and want to know what simple models achieve.\n",
    "\n",
    "* If your system gets 0.60 F1, you might despair, but it could turn out that humans achieve only 0.80, indicating that you got traction on a very challenging but basically coherent problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Baselines are crucial for strong experiments\n",
    "\n",
    "Defining baselines should not be an afterthought, but rather central to how you define your overall hypotheses. __Baselines are essential to building a persuasive case__, and they can also be used to illuminate specific aspects of the problem and specific virtues of your proposed system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random baselines\n",
    "\n",
    "Random baselines are almost always useful to include. scikit-learn has classes [DummyClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier) and [DummyRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor). Each of them has a keyword argument `strategy` that allows you to specify a range of different styles of random guessing. I highly recommend using these in your work, for two central reasons:\n",
    "\n",
    "1. They will probably fit into your overall modeling pipeline.\n",
    "2. It's usual conceptually easy to describe these baselines but it can be tricky and error-prone to implement them – and the scikit-learn folks probably already did it for you flawlessly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task-specific baselines\n",
    "\n",
    "It is worth considering whether your problem suggests a baseline that will reveal something about the problem or the ways it is modeled. Two recent examples from NLU:\n",
    "\n",
    "1. As disussed briefly in [the NLI models notebook](nli_02_models.ipynb#Other-findings), [Leonid Keselman](https://leonidk.com/) observed [in his 2016 NLU course project](https://leonidk.com/stanford/cs224u.html) that one can do much better than chance on SNLI by processing only the hypothesis, ignoring the premise entirely. The exact interpretation of this is complex (we explore this a bit [in our NLI unit](nli_02_models.ipynb#Hypothesis-only-baselines) and [in our NLI bake-off](nli_wordentail.ipynb)), but it's certainly relevant for understanding how much a system has actually learned about reasoning from a premise to a conclusion.\n",
    " \n",
    "1. [Schwartz et al. (2017)](https://www.aclweb.org/anthology/W17-0907) develop a system for choosing between a coherent and incoherent ending for a story. Their best system achieves 75% accuracy by processing the story and the ending, but they achieve 72% using only stylistic features of the ending, ignoring the preceding story entirely. This puts the 75% – and the extent to which the system understands story completion – in a new light."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "In machine learning, the __parameters__ of a model are those whose values are learned as part of optimizing the model itself. \n",
    "\n",
    "The __hyperparameters__ of a model are any settings that are set by a process that is outside of this optimization process. The boundary between a true setting of the model and a broader design choice will likely be blurry conceptually. For example: \n",
    "\n",
    "* The regularization term for a classifier is a clear hyperparameter – it appears in the model's objective function. \n",
    "* What about the method one uses for normalizing the feature values? This is probably not a setting of the model per se, but rather a choice point in your experimental framework.\n",
    "  \n",
    "For the purposes of this discussion, we'll construe hyperparameters very broadly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rationale\n",
    "\n",
    "Hyperparameter optimization is one of the most important parts machine learning, and a crucial part of building a persuasive argument. For one angle on why, it's helpful to imagine that you're in an ongoing debate with a very skeptical referee:\n",
    "\n",
    "1. You ran experiments with models A, B, and C. For each, you used the default hyperparameters as given by the implementations you're using. You found that C performed the best, and so you reported that in your paper.\n",
    "1. Your reviewer doesn't have visibility into your process, and maybe doesn't fully trust you. Did you try any other values for the hyperparameters without reporting that? If not, would you have done that if C hadn't outperformed the others? There is no way for the reviewer (or perhaps anyone) to answer these questions.\n",
    "1. So, from the reviewer's perspective, all we learned from your experiments is that there is some set of hyperparameters on which C wins this competition. But, strictly speaking, this conveys no new information; we knew before you did your experiments that we could find settings that would deliver this and all other outcomes. (They might not be __sensible__ settings, but remember you're dealing with a hard-bitten, unwavering skeptic.)\n",
    "\n",
    "Our best response to this situation is to allow these models to explore a wide range of hyperparameters, choose the best ones according to performance on training or development data, and then report how they do with those settings at test time. __This gives every model its best chance to succeed.__\n",
    "\n",
    "If you do this, the strongest argument that your skeptical reviewer can muster is that you didn't pick the right space of hyperparameters to explore for one or more of the models. Alas, there is no satisfying the skeptic, but we can at least feel happy that the outcome of these experiments will have a lot more scientific value than the ones described above with fixed hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The ideal hyperparameter optimization setting\n",
    "\n",
    "When evaluating a model, the ideal regime for hyperparameter optimization is as follows:\n",
    "\n",
    "1. For each hyperparameter, identify a large set of values for it. \n",
    "2. Create a list of all the combinations of all the hyperparameter values. This will be the [cross-product](https://en.wikipedia.org/wiki/Cartesian_product) of all the values for all the features identified at step 1.\n",
    "3. For each of the settings, cross-validate it on the available training data.\n",
    "4. Choose the settings that did best in step 3, train on all the training data using those settings, and then evaluate that model on the test set.\n",
    "\n",
    "This is very demanding. First, The number of settings grows quickly with the number of hyperparameters and values. If hyperparameter $h_{1}$ has $5$ values and hyperparameter $h_{2}$ has $10$, then the number of settings is $5 \\cdot 10 = 50$. If we add a third hyperparameter $h_{3}$ with just $2$ values, then the number jumps to $100$. Second, if you're doing 5-fold cross-validation, then each model is trained 5 times. You're thus committed to training $500$ models.\n",
    "\n",
    "And it could get worse. Suppose you don't have a fixed train/test split, and you're instead reporting, say, the result of 10 random train/test splits. Strictly speaking, the optimal hyperparameters could be different for different splits. Thus, for each split, the above cross-validation should be conducted. Now you're committed to training $5,000$ systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practical considerations, and some compromises\n",
    "\n",
    "The above is untenable as a set of laws for the scientific community. If we adopted it, then complex models trained on large datasets would end up disfavored, and only the very wealthy would be able to participate. Here are some pragmatic steps you can take to alleviate this problem, in descending order of attractiveness. (That is, the lower you go on this list, the more likely the skeptic is to complain!)\n",
    "\n",
    "1. [Bergstra and Bengio (2012)](http://www.jmlr.org/papers/v13/bergstra12a.html) argue that __randomly sampling__ from the space of hyperparameters delivers results like the full \"grid search\" described above with a relatively few number of samples. __Hyperparameter optimization algorithms__ like those implemented in [Hyperopt](http://hyperopt.github.io/hyperopt/) and [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize) allow guided sampling from the full space. All these methods control the exponential growth in settings that comes from any serious look at one's hyperparameters. \n",
    "\n",
    "1. In large deep learning systems, __the hyperparameter search could be done on the basis of just a few iterations__. The systems likely won't have converged, but it's a solid working assumption that early performance is highly predictive of final performance. You might even be able to justify this with learning curves over these initial iterations.\n",
    "\n",
    "1. Not all hyperparameters will contribute equally to outcomes. Via heuristic exploration, it is typically possible to __identify the less informative ones and set them by hand__. As long as this is justified in the paper, it shouldn't rile the skeptic too much.\n",
    "\n",
    "1. Where repeated train/test splits are being run, one might __find optimal hyperparameters via a single split__ and use them for all the subsequent splits. This is justified if the splits are very similar.\n",
    "\n",
    "1. In the worst case, one might have to adopt hyperparameters that were optimal for other experiments that have been published. The skeptic will complain that these findings don't translate to your new data sets. That's true, but it could be the only option. For example, how would one compare against [Rajkomar et al. (2018)](https://arxiv.org/abs/1801.07860) who report that \"the performance of all above neural networks were [sic] tuned automatically using Google Vizier [35] with a total of >201,000 GPU hours\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameter optimization tools\n",
    "\n",
    "* scikit-learn's [model_selection](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) package has classes `GridSearchCV` and `RandomizedSearchCV`. These are very easy to use. (We used `GridSearchCV` in our course code, in `utils.fit_classifier_with_hyperparameter_search`.)\n",
    "\n",
    "* [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize) offers a variety of methods for guided search through the grid of hyperparameters. [This post](https://roamanalytics.com/2016/09/15/optimizing-the-hyperparameter-of-which-hyperparameter-optimizer-to-use/) assesses these methods against grid search and fully randomized search, and it also provides [starter code](https://github.com/roamanalytics/roamresearch/tree/master/BlogPosts/Hyperparameter_tuning_comparison) for using these implementations with sklearn-style classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier comparison\n",
    "\n",
    "Suppose you've assessed two classifier models. Their performance is probably different to some degree. What can be done to establish whether these models are different in any meaningful sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practical differences\n",
    "\n",
    "One very simple step one can take is to simply count up how many examples the models actually differ on. \n",
    "\n",
    "* If the test set has 1,000 examples, then a difference of 1% in accuracy or F1 will correspond to roughly 10 examples. We'll likely have intuitions about whether that difference has any practical import. \n",
    "\n",
    "* If the test set has 1M examples, then 1% will correspond to 10,000 examples, which seems sure to matter. Unless other considerations (e.g., cost, understandability) favor the less accurate model, the choice seems clear.\n",
    "\n",
    "Even where the numbers suggest a practical difference, we might still wonder whether the difference is stable across different runs, and thus we might still want to gather more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence intervals\n",
    "\n",
    "If you can afford to run the model multiple times, then reporting confidence intervals based on the resulting scores could suffice to build an argument about whether the models are meaningfully different.\n",
    "\n",
    "The following will calculate a simple 95% confidence interval for a vector of scores `vals`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ci(vals):\n",
    "    if len(set(vals)) == 1:\n",
    "        return (vals[0], vals[0])\n",
    "    loc = np.mean(vals)\n",
    "    scale = np.std(vals) / np.sqrt(len(vals))\n",
    "    return stats.t.interval(0.95, len(vals)-1, loc=loc, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very likely that these confidence intervals will look very large relative to the variation that you actually observe. You probably can afford to do no more than 10–20 runs. Even if your model is performing very predictably over these runs (which it will, assuming your method for creating the splits is sound), the above intervals will be large in this situation. This might justify bootstrapping the confidence intervals. I recommend [scikits-bootstrap](https://github.com/cgevans/scikits-bootstrap) for this.\n",
    "\n",
    "__Important__: when evaluating multiple systems via repeated train/test splits or cross-validation, all the systems have to be run on the same splits. This is the only way to ensure that all the systems face the same challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wilcoxon signed-rank test\n",
    "\n",
    "NLPers always choose tables over plots for some reason, and confidence intervals are hard to display in tables. This might mean that you want to calculate a p-value. \n",
    "\n",
    "Where you can afford to run the models at least 10 times with different splits (and preferably more like 20), [Demšar (2006)](http://www.jmlr.org/papers/v7/demsar06a.html) recommends the [Wilcoxon signed-rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test). This is implemented in scipy as [scipy.stats.wilcoxon](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html). This test relies only on the absolute differences between scores for each split and makes no assumptions about how the scores are distributed.\n",
    "\n",
    "Take care not to confuse this with [scipy.stats.ranksums](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ranksums.html), which does the Wilcoxon rank-sums test. This is also known as the [Mann–Whitney U test](https://en.wikipedia.org/wiki/Mann–Whitney_U_test), though SciPy distinguishes this as a separate test ([scipy.stats.mannwhitneyu](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html#scipy.stats.mannwhitneyu)). In any case, the heart of this is that the signed-rank variant is more appropriate for classifier assessments, where we are always comparing systems trained and assessed on the same underlying pool of data.\n",
    "\n",
    "Like all tests of this form, we should be aware of what they can tell us and what they can't: \n",
    "\n",
    "* The test says __nothing__ about the practical importance of any differences observed. \n",
    "\n",
    "* __Small p-values do not reliably indicate large effect sizes__. (A small p-value will more strongly reflect the number of samples you have.)\n",
    "\n",
    "* Large p-values simply mean that the available evidence doesn't support a conclusion that the systems are different, not that there is no difference in fact. And even that limited conclusion is only relative to this particular, quite conservative test. \n",
    "\n",
    "All this is to say that these values should not be asked to stand on their own, but rather presented as part of a larger, evidence-driven argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### McNemar's test\n",
    "\n",
    "[McNemar's test](https://en.wikipedia.org/wiki/McNemar%27s_test) operates directly on the vectors of predictions for the two models being compared. As such, it doesn't require repeated runs, which is good where optimization is expensive.\n",
    "\n",
    "The basis for the test is a contingency table with the following form, for two models A and B:\n",
    "\n",
    "$$\\begin{array}{|c | c |}\n",
    "\\hline\n",
    "\\textrm{number of examples}        & \\textrm{number of examples} \\\\\n",
    "\\textrm{where A and B are correct} & \\textrm{where A is correct, B incorrect} \n",
    "\\\\\\hline\n",
    "\\textrm{number of examples}        & \\textrm{number of examples} \\\\\n",
    "\\textrm{where A is correct, B incorrect} & \\textrm{where both A and B are incorrect} \\\\\\hline\n",
    "\\end{array}$$\n",
    "\n",
    "Following [Dietterich (1998)](http://sci2s.ugr.es/keel/pdf/algorithm/articulo/dietterich1998.pdf), let the above be abbreviated to\n",
    "\n",
    "$$\\begin{array}{|c | c |}\n",
    "\\hline\n",
    "n_{11} & n_{10}\n",
    "\\\\\\hline\n",
    "n_{01} & n_{00} \\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "\n",
    "The null hypothesis tested is that the two models have the same error rate, i.e., that $n_{01} = n_{10}$. The test statistic is\n",
    "\n",
    "$$\n",
    "\\frac{\n",
    "    \\left(|(n_{01} - n_{10}| - 1\\right)^{2}\n",
    "}{\n",
    "    n_{01} + n_{10}\n",
    "}$$\n",
    "\n",
    "which has an approximately chi-squared distribution with 1 degree of freedom. \n",
    "\n",
    "An implementation is available in this repository: `utils.mcnemar`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assessing models without convergence\n",
    "\n",
    "When working with linear models, convergence issues rarely arise. Typically, the implementation has a fixed number of iterations it performs, or a threshold on the error, and the model stops when it reaches one of these points. We mostly don't reflect on this because of the speed and stability of these models.\n",
    "\n",
    "With neural networks, convergence takes center stage. The models rarely converge, or they converge at different rates between runs, and their performance on the test data is often heavily dependent on these differences. Sometimes a model with a low final error turns out to be great, and sometimes it turns out to be worse than one that finished with a higher error. Who knows?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Incremental dev set testing\n",
    "\n",
    "The key to addressing this uncertainty is to __regularly collect information about dev set performance as part of training__. For example, after every epoch, one could make predictions on the dev set and store that vector of predictions, or just whatever assessment metric one is using. These assessments can provide direct information about how the model is doing on the actual task we care about, which will be a better indicator than the errors.\n",
    "\n",
    "All the PyTorch models for this course accept a keyword argument `early_stopping`. The behavior should closely resemble that of `sklearn.neural_network` models. If `early_stopping=True`, then part of the dataset given to the `fit` method is reserved for incremental testing. The amount can be controlled with `validation_fraction` (default: `0.10`). After every epoch, this data will be used to evaluate the model using its `score` method. The parameters of the best model are stored. If an improvement of more than `tol` (default: `1e-5`) isn't seen within `n_iter_no_change` steps (default: `10`), then optimization stops, and the parameters of the numerically best model seen are used as the final model.\n",
    "\n",
    "It's important to see just how different this dev set performance can be from the training loss. In particular, the training loss can continue to go down even as the model grows worse and worse in evaluations on held-out data. This is a common form of __over-fitting__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dataset to illustrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_HOME = os.path.join(\"data\", \"trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(tree):\n",
    "    return Counter(tree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sst.build_dataset(\n",
    "    SST_HOME,\n",
    "    reader=sst.train_reader,\n",
    "    class_func=sst.binary_class_func,\n",
    "    phi=unigrams_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = sst.build_dataset(\n",
    "    SST_HOME,\n",
    "    reader=sst.dev_reader,\n",
    "    class_func=sst.binary_class_func,\n",
    "    phi=unigrams_phi,\n",
    "    vectorizer=train['vectorizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model without early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mod_no_stopping = TorchShallowNeuralClassifier(\n",
    "    early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 447. Training loss did not improve more than tol=1e-05. Final error is 0.000899996972293593."
     ]
    }
   ],
   "source": [
    "_ = mod_no_stopping.fit(train['X'], train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.765     0.729     0.746       428\n",
      "    positive      0.750     0.784     0.767       444\n",
      "\n",
      "    accuracy                          0.757       872\n",
      "   macro avg      0.757     0.756     0.756       872\n",
      "weighted avg      0.757     0.757     0.757       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev['y'], mod_no_stopping.predict(dev['X']), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_stopping = TorchShallowNeuralClassifier(\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 69. Validation score did not improve by tol=1e-05 for more than 50 epochs. Final error is 0.06454353779554367"
     ]
    }
   ],
   "source": [
    "_ = mod_stopping.fit(train['X'], train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.781     0.734     0.757       428\n",
      "    positive      0.757     0.802     0.779       444\n",
      "\n",
      "    accuracy                          0.768       872\n",
      "   macro avg      0.769     0.768     0.768       872\n",
      "weighted avg      0.769     0.768     0.768       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev['y'], mod_stopping.predict(dev['X']), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Errors vs. incremental performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = mod_stopping.validation_scores\n",
    "errors = mod_no_stopping.errors[: len(scores)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1 = pd.Series(scores).plot(ax=ax1)\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Macro F1 score\")\n",
    "\n",
    "ax2 = pd.Series(errors).plot(ax=ax2)\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "_ = ax2.set_ylabel(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning curves with confidence intervals\n",
    "\n",
    "I frankly think the best response to all this is to accept that incremental performance plots like the above are how we should be assessing our models. This exposes all of the variation that we actually observe. \n",
    "\n",
    "In addition, in deep learning, we're often dealing with classes of models that are in principle capable of learning anything. The real question is implicitly how efficiently they can learn given the available data and other resources. Learning curves bring this our very clearly.\n",
    "\n",
    "We can improve the curves by adding confidence intervals to them derived from repeated runs. Here's a plot from a paper I recently wrote with Nick Dingwall ([Dingwall and Potts 2018](https://arxiv.org/abs/1803.09901)):\n",
    "\n",
    "<img src=\"fig/diagnosis-curve.png\" />\n",
    "\n",
    "I think this shows very clearly that, once all is said and done, the Mittens model (red) learns faster than the others, but is indistinguishable from the Clinical text GloVe model (blue) after enough training time. Furthermore, it's clear that the other two models are never going to catch up in the current experimental setting. A lot of this information would be lost if, for example, we decided to stop training when dev set performance reached its peak and report only a single F1 score per class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The role of random parameter initialization\n",
    "\n",
    "Most deep learning models have their parameters initialized randomly, perhaps according to some heuristics related to the number of parameters ([Glorot and Bengio 2010](http://proceedings.mlr.press/v9/glorot10a.html)) or their internal structure ([Saxe et al. 2014](https://arxiv.org/abs/1312.6120)). This is meaningful largely because of the non-convex optimization problems that these models define, but it can impact simpler models that have multiple optimal solutions that still differ at test time. \n",
    "\n",
    "There is growing awareness that these random choices have serious consequences. For instance, [Reimers and Gurevych (2017)](https://www.aclweb.org/anthology/D17-1035) report that different initializations for neural sequence models can lead to statistically significant results, and they show that a number of recent systems are indistinguishable in terms of raw performance once this source of variation is taken into account.\n",
    "\n",
    "This shouldn't surprise practitioners, who have long struggled with the question of what to do when a system experiences a catastrophic failure as a result of unlucky initialization. (I think the answer is to report this failure rate.)\n",
    "\n",
    "The code snippet below lets you experience this phenomenon for yourself. The XOR logic operator, which is true just in case its two arguments have the same value, is famously not learnable by a linear classifier but within reach of a neural network with a single hidden layer and a non-linear activation function ([Rumelhart et al. 1986](https://www.nature.com/articles/323533a0)). But how consistently do such models actually learn XOR? No matter what settings you choose, you rarely if ever see perfect performance across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 500 of 500; error is 0.007755517493933439ore than tol=1e-05. Final error is 0.6944566369056702."
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'correct': 9, 'incorrect': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xor_eval(n_trials=10):\n",
    "    xor = [\n",
    "        ([1.,1.], 1),\n",
    "        ([1.,0.], 0),\n",
    "        ([0.,1.], 0),\n",
    "        ([0.,0.], 1)]\n",
    "    X, y = zip(*xor)\n",
    "    results = defaultdict(int)\n",
    "    for trial in range(n_trials):\n",
    "        model = TorchShallowNeuralClassifier(\n",
    "            hidden_dim=2,\n",
    "            max_iter=500,\n",
    "            eta=0.01)\n",
    "        model.fit(X, y)\n",
    "        preds = tuple(model.predict(X))\n",
    "        result = 'correct' if preds == y else 'incorrect'\n",
    "        results[result] += 1\n",
    "    return results\n",
    "\n",
    "xor_eval(n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For better or worse, the only response we have to this situation is to __report scores for multiple complete runs of a model with different randomly chosen initializations__. [Confidence intervals](#Confidence-intervals) and [statistical tests](#Wilcoxon-signed-rank-test) can be used to summarize the variation observed. If the evaluation regime already involves comparing the results of multiple train/test splits, then ensuring a new random initializing for each of those would seem sufficient.\n",
    "\n",
    "Arguably, these observations are incompatible with evaluation regimes involving only a single train/test split, as in [McNemar's test](#McNemar's-test). However, [as discussed above](#Practical-considerations,-and-some-compromises), we have to be realistic. If multiple run aren't feasible, then a more heuristic argument will be needed to try to convince skeptics that the differences observed are larger than we would expect from just different random initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Closing remarks\n",
    "\n",
    "We can summarize most of the above with a few key ideas:\n",
    "    \n",
    "1. Your evaluation should be based around a few systems that are related in ways that illuminate your hypotheses and help to convey what the best models are learning.\n",
    "\n",
    "1. Every model you assess should be given its best chance to shine (but we need to be realistic about how many experiments this entails!).\n",
    "\n",
    "1. The test set should play no role whatsoever in optimization or model selection. The best way to ensure this is to have the test set locked away until the final batch of experiments that will be reported in the paper, but this separation is simulated adequately by careful cross-validation set-ups.\n",
    "\n",
    "1. Strive to base your model comparisons in multiple runs on the same splits. This is especially important for deep learning, where a single model can perform in very different ways on the same data, depending on the vagaries of optimization."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
